{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82eb0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "import scipy\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "import itertools\n",
    "import glob\n",
    "import re\n",
    "\n",
    "package_path = os.path.join(pathlib.Path.home(), \"mkvchain\")\n",
    "sys.path.append(package_path)\n",
    "from model import FeatureDependentMarkovChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ab866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = pathlib.Path.home()\n",
    "# pt = os.path.join(root, 'research', 'GADS_Processed', \"trajectories_with_features\")\n",
    "\n",
    "# def glob_re(pattern, strings):\n",
    "#     return list(filter(re.compile(pattern).match, strings))\n",
    "\n",
    "# filenames = glob_re(r\"gen_\\d+_type_200_dtgrp_\\d+_rating_\\d+_state_Texas_raw.csv\", os.listdir(pt))\n",
    "\n",
    "# mylist = []\n",
    "\n",
    "# for f in filenames:\n",
    "#     data = pd.read_csv(os.path.join(pt,f))\n",
    "#     data.set_index(pd.DatetimeIndex(data[\"x\"]), inplace=True)\n",
    "#     data = data[~data.index.duplicated()]\n",
    "#     data.rename(columns={\"COAST\": \"y5\", \"NORTH_C\" : \"y5\"}, inplace=True)\n",
    "#     mylist.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cf61354",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = pathlib.Path.home()\n",
    "pt1 = os.path.join(root, 'research', 'GADS_Processed', \"trajectories_with_features\")\n",
    "pt2 = os.path.join(root, 'research', 'ERCOT_WEATHER_DATA', \"processed\", \"texas_coast_weather_daily_2013_2023.csv.csv\")\n",
    "weather = pd.read_csv(pt2)\n",
    "weather.set_index(pd.DatetimeIndex(weather[\"DATE\"]), inplace=True)\n",
    "\n",
    "def glob_re(pattern, strings):\n",
    "    return list(filter(re.compile(pattern).match, strings))\n",
    "\n",
    "filenames = glob_re(r\"gen_\\d+_type_200_dtgrp_\\d+_rating_\\d+_state_Texas_raw.csv\", os.listdir(pt1))\n",
    "\n",
    "mylist = []\n",
    "\n",
    "for f in filenames:\n",
    "    data = pd.read_csv(os.path.join(pt1,f))\n",
    "    data.set_index(pd.DatetimeIndex(data[\"x\"]), inplace=True)\n",
    "    data = data[~data.index.duplicated()]\n",
    "    # data.rename(columns={\"COAST\": \"y5\", \"NORTH_C\" : \"y5\"}, inplace=True)\n",
    "    data = data.join(weather)\n",
    "    mylist.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f0eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in mylist:  \n",
    "    # po_idx = np.where(np.logical_or(d[\"y2\"].values == 3, d[\"y2\"].values == 4))[0]\n",
    "    po_idx = np.where(d[\"y2\"].values == 3)[0]\n",
    "    end_of_event = np.concat([[False], np.diff(po_idx) != 1])\n",
    "    end_of_event_idx = po_idx[end_of_event]\n",
    "\n",
    "    idx = range(len(d))\n",
    "    my_data = np.zeros(len(d))\n",
    "\n",
    "    for i in idx:\n",
    "        events_before_i = np.array([j for j in end_of_event_idx if j < i])\n",
    "        if len(events_before_i) > 1:\n",
    "            closest_idx = np.argmin(i - events_before_i)\n",
    "            my_event_idx = events_before_i[closest_idx]\n",
    "            hours_since_event = (i - my_event_idx)\n",
    "        else:\n",
    "            hours_since_event = None\n",
    "        my_data[i] = hours_since_event\n",
    "    \n",
    "    d[\"y6\"] = my_data\n",
    "\n",
    "    # mo_idx = np.where(d[\"y2\"].values == 4)[0]\n",
    "    # if len(mo_idx) < 1:\n",
    "    #     my_data = None\n",
    "    # else:\n",
    "    #     end_of_event = np.concat([[False], np.diff(mo_idx) != 1])\n",
    "    #     end_of_event_idx = mo_idx[end_of_event]\n",
    "\n",
    "    #     idx = range(len(d))\n",
    "    #     my_data = np.zeros(len(d))\n",
    "\n",
    "    #     for i in idx:\n",
    "    #         events_before_i = np.array([j for j in end_of_event_idx if j < i])\n",
    "    #         if len(events_before_i) > 1:\n",
    "    #             closest_idx = np.argmin(i - events_before_i)\n",
    "    #             my_event_idx = events_before_i[closest_idx]\n",
    "    #             hours_since_event = (i - my_event_idx)\n",
    "    #         else:\n",
    "    #             hours_since_event = None\n",
    "    #         my_data[i] = hours_since_event\n",
    "    \n",
    "    # d[\"y7\"] = my_data\n",
    "\n",
    "    d[\"y8\"] = d.index.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a227e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# features = d.loc[(~np.isnan(d[\"y6\"])) & (~np.isnan(d[\"y5\"])), [\"y3\", \"y5\", \"y4\", \"y6\"]]\n",
    "# states = d.loc[features.index, \"y2\"].values\n",
    "\n",
    "# print(sklearn.feature_selection.r_regression(features, states))\n",
    "# print(sklearn.feature_selection.chi2(features, states))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eae58f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y3 0.0\n",
      "COAST 0.00015329933491778217\n",
      "y4 0.0\n",
      "PRCP 0.0\n",
      "TMAX 0.0\n",
      "TMIN 0.0\n",
      "y6 0.03197595686923424\n",
      "y8 0.0\n"
     ]
    }
   ],
   "source": [
    "d = mylist[0]\n",
    "for c in ['y3', 'COAST', 'y4', 'PRCP', 'TMAX', 'TMIN', 'y6', 'y7', 'y8']:\n",
    "    contingency_table = pd.crosstab(d[\"y2\"], d[c])\n",
    "    try:\n",
    "        res = scipy.stats.chi2_contingency(contingency_table)\n",
    "    except:\n",
    "        continue\n",
    "    print(c, res.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff60d8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y3 0.0\n",
      "COAST 0.010746434739238293\n",
      "y4 0.0\n",
      "PRCP 0.0\n",
      "TMAX 0.0\n",
      "TMIN 0.0\n",
      "y6 0.0\n",
      "y8 0.0\n"
     ]
    }
   ],
   "source": [
    "d = mylist[1]\n",
    "for c in ['y3', 'COAST', 'y4', 'PRCP', 'TMAX', 'TMIN', 'y6', 'y7', 'y8']:\n",
    "    contingency_table = pd.crosstab(d[\"y2\"], d[c])\n",
    "    try:\n",
    "        res = scipy.stats.chi2_contingency(contingency_table)\n",
    "    except:\n",
    "        continue\n",
    "    print(c, res.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9217969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x\n",
       "2013-01-01 06:00:00    None\n",
       "2013-01-01 07:00:00    None\n",
       "2013-01-01 08:00:00    None\n",
       "2013-01-01 09:00:00    None\n",
       "2013-01-01 10:00:00    None\n",
       "                       ... \n",
       "2023-12-31 20:00:00    None\n",
       "2023-12-31 21:00:00    None\n",
       "2023-12-31 22:00:00    None\n",
       "2023-12-31 23:00:00    None\n",
       "2024-01-01 00:00:00    None\n",
       "Name: y7, Length: 96392, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['y7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66c9ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_new = []\n",
    "features_new = []\n",
    "lengths_new = []\n",
    "\n",
    "for d in mylist:\n",
    "    states = d[\"y2\"].values.tolist()\n",
    "    features = d[['y3', 'COAST', 'y4', 'PRCP', 'TMAX', 'TMIN', 'y6', 'y8']].values\n",
    "    l = len(d)\n",
    "\n",
    "    states_new += [states]\n",
    "    features_new += [features]\n",
    "    lengths_new += [l]\n",
    "\n",
    "states = np.concatenate(states_new).astype(int)\n",
    "states -= 1\n",
    "features = np.vstack(features_new)\n",
    "lengths = np.array(lengths_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "860aefd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a09a1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = 1 # int(lengths.size * .8)\n",
    "val_idx = 2 #int(lengths.size * .9)\n",
    "\n",
    "lengths_train = lengths[:train_idx]\n",
    "lengths_val = lengths[train_idx:val_idx]\n",
    "# lengths_test = lengths[val_idx:]\n",
    "\n",
    "states_train = states[:lengths_train.sum()]\n",
    "states_val = states[lengths_train.sum():lengths_train.sum()+lengths_val.sum()]\n",
    "# states_test = states[lengths_train.sum()+lengths_val.sum():]\n",
    "\n",
    "features_train = features[:lengths_train.sum()]\n",
    "features_val = features[lengths_train.sum():lengths_train.sum()+lengths_val.sum()]\n",
    "# features_test = features[lengths_train.sum()+lengths_val.sum():]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69c16f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "features_train = ss.fit_transform(features_train)\n",
    "features_val = ss.transform(features_val)\n",
    "# features_test = ss.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ff647e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.73227784, -1.79290221, -1.72762601, ..., -0.33909146,\n",
       "                nan, -1.6020725 ],\n",
       "       [-1.73227784, -1.74857076, -1.72762601, ..., -0.33909146,\n",
       "                nan, -1.6020725 ],\n",
       "       [-1.73227784, -1.73272132, -1.72762601, ..., -0.33909146,\n",
       "                nan, -1.6020725 ],\n",
       "       ...,\n",
       "       [ 1.73200392, -0.28931338,  1.72547345, ..., -1.97503395,\n",
       "        -0.71249848,  1.58788438],\n",
       "       [ 1.73200392, -0.38119227,  1.72547345, ..., -1.97503395,\n",
       "        -0.71199974,  1.58788438],\n",
       "       [ 1.73200392, -0.47795592, -1.72762601, ..., -1.54826635,\n",
       "        -0.711501  , -1.6020725 ]], shape=(96392, 8))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1be8ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-773.996324337362), np.float64(-593.8767097560176))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 4\n",
    "model1 = FeatureDependentMarkovChain(n, lam_frob=0, n_iter=1)\n",
    "model1.fit(states_train, features_train*0, lengths_train, verbose=False)\n",
    "train1, val1 = model1.score(states_train, features_train*0, lengths_train, average=False), \\\n",
    "    model1.score(states_val, features_val*0, lengths_val, average=False)\n",
    "    # model1.score(states_test, features_test*0, lengths_test, average=False)\n",
    "train1, val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aa01ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> 0 : 2\n",
      "0 -> 1 : 2\n",
      "0 -> 2 : 2\n",
      "0 -> 3 : 2\n",
      "1 -> 0 : 2\n",
      "1 -> 1 : 2\n",
      "1 -> 2 : 2\n",
      "1 -> 3 : 2\n",
      "2 -> 0 : 2\n",
      "2 -> 1 : 2\n",
      "2 -> 2 : 2\n",
      "2 -> 3 : 2\n",
      "3 -> 0 : 2\n",
      "3 -> 1 : 2\n",
      "3 -> 2 : 2\n",
      "3 -> 3 : 2\n"
     ]
    }
   ],
   "source": [
    "predictions = model1.predict(features_val)\n",
    "for i, j in itertools.product(range(4), range(4)):\n",
    "    y = predictions[:, i, j]\n",
    "    val = len(np.unique(y))\n",
    "    print(f\"{i} -> {j} : {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d4a1117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-762.7624227693096), np.float64(-584.0683045673397))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "train2, val2, test2 = -np.inf, -np.inf, -np.inf\n",
    "best_lam = None\n",
    "model2 = None\n",
    "for lam in np.logspace(-3,-1,10):\n",
    "    model = FeatureDependentMarkovChain(n, lam_frob=lam,  n_iter=1)\n",
    "    model.As = deepcopy(model1.As)\n",
    "    model.bs = deepcopy(model1.bs)\n",
    "    model.fit(states_train, features_train, lengths_train, verbose=False, warm_start=True)\n",
    "    traini, vali= model.score(states_train, features_train, lengths_train, average=False), \\\n",
    "          model.score(states_val, features_val, lengths_val, average=False)\n",
    "        #   model.score(states_test, features_test, lengths_test, average=False)\n",
    "    if vali > val2:\n",
    "        train2 = traini\n",
    "        val2 = vali\n",
    "        # test2 = testi\n",
    "        best_lam = lam\n",
    "        model2 = model\n",
    "train2, val2# test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccd3f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeatureDependentMarkovChain(n, lam_frob=best_lam, n_iter=1)\n",
    "model.As = deepcopy(model1.As)\n",
    "model.bs = deepcopy(model1.bs)\n",
    "model.fit(states_train, features_train, lengths_train, verbose=False, warm_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2229c768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> 0 : 9\n",
      "0 -> 1 : 4\n",
      "0 -> 2 : 6\n",
      "0 -> 3 : 2\n",
      "1 -> 0 : 255\n",
      "1 -> 1 : 263\n",
      "1 -> 2 : 7\n",
      "1 -> 3 : 2\n",
      "2 -> 0 : 41\n",
      "2 -> 1 : 2\n",
      "2 -> 2 : 41\n",
      "2 -> 3 : 2\n",
      "3 -> 0 : 11\n",
      "3 -> 1 : 2\n",
      "3 -> 2 : 2\n",
      "3 -> 3 : 11\n"
     ]
    }
   ],
   "source": [
    "predictions2 = model.predict(features_val)\n",
    "for i, j in itertools.product(range(4), range(4)):\n",
    "    y = predictions2[:, i, j]\n",
    "    val = len(np.unique(y.round(decimals=4)))\n",
    "    print(f\"{i} -> {j} : {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93924469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['y8' 'TMAX' 'PRCP' 'y3' 'COAST']\n",
      "['y6' 'TMIN' 'y4' 'y8' 'TMAX']\n"
     ]
    }
   ],
   "source": [
    "j = np.where(model.nonzero[0] == 0)[0][0]\n",
    "A, b = model.As[0], model.bs[0]\n",
    "feature_names = np.array(['y3', 'COAST', 'y4', 'PRCP', 'TMAX', 'TMIN', 'y6', 'y8'])\n",
    "print(feature_names[np.argsort(A[:,j])[-5:]]) # month, TMAX', 'PRCP', age, load\n",
    "print(feature_names[np.argsort(A[:,j])[:5]]) # time since last planned outage, doy, month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f3f781c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(model.nonzero[0] == 0)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac35cde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cxgrp",
   "language": "python",
   "name": "cxgrp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
